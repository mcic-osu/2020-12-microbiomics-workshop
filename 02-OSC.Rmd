---
title: "Introduction to OSC"
author: "Jelmer Poelstra"
date: "9/30/2020"
output:
  ioslides_presentation:
    widescreen: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## About OSC

- Statewide resource for all universities in Ohio (**not** a part of OSU!)

<br/>

- To make use of OSC, you need to have access to a *project*.
  - PIs can request a project, and since Summer 2020, OSC charges directly for storage (per TB per month) and compute (core hours), see the [Academic Fee Model FAQ](https://www.osc.edu/content/academic_fee_model_faq).

## Introductory materials

- Excellent material to get started the [Getting Started Page](https://www.osc.edu/resources/getting_started)

<p align="center">
<img src=figs/gettingstarted.png width="600">
</p>

## Introductory materials

- Excellent material to get started the [Getting Started Page](https://www.osc.edu/resources/getting_started)
  - Have a look at all the ["HOWTO" pages](https://www.osc.edu/resources/getting_started/howto) that includes more advanced material, too.

<br/>

- Online introductory sessions are regularly held, see the [OSC Events page](https://www.osc.edu/events) page.
  - [Here](https://www.osc.edu/sites/default/files/staff_files/kcahill/NewUser_OSU_090419_post.pdf) is the presentation from a 2019 session (quite some changes have taken place since then, so a bit outdated.)

<br/>

- There is also a Carpentry-style tutorial available [here](https://khill42.github.io/OSC_IntroHPC).

## Clusters at OSC

<p align="center">
<img src=figs/computers.png width="450">
</p>

- You can ignore Ruby (will be decommissioned at the end of 2020)
- Pitzer just got an expansion (and now runs the SLURM scheduler)

## Connecting to OSC with "OnDemand"

- You can make use of OSC not only through ssh at the command line, but also through
the web browser, from http://ondemand.osc.edu

<p align="center">
<img src=figs/ondemand1.png width="900">
</p>

<br/>

- Here you can browse your files, and submit and manage jobs -- similar functionality to the command-line, but visual.

- Especially useful are some of the "Interactive Apps", such as RStudio.

## Connecting to use with `ssh`

- Basic usage -- when you log in, it's to a specific cluster:
```bash
$ ssh <username>@pitzer.osc.edu        
$ ssh <username>@owens.osc.edu
```
<br/>

- You will be prompted for your OSC password.

## `ssh` set-up: avoid being prompted for password {.smaller}

- On your own computer, generate a key:
```bash
$ ssh-keygen -t rsa
```

- On your own computer, transfer the key to the remote computer:
```bash
$ cat ~/.ssh/id_rsa.pub | ssh <user>@owens.osc.edu 'mkdir -p .ssh && cat >> .ssh/authorized_keys'
```

- On remote computer (OSC), set permissions:
```bash
$ chmod 700 .ssh; chmod 640 .ssh/authorized_keys
```

- For more details, see this [Tecmint post](https://www.tecmint.com/ssh-passwordless-login-using-ssh-keygen-in-5-easy-steps/).

## `ssh` set-up: use shortcuts

- Create a file called `~/.ssh/config`:
```bash
$ touch `~/.ssh/config`
```

- Open the file and add your alias(es):
```bash
Host <arbitrary-alias-name>    
  HostName <remote-name>
  User <user-name>
```

- This is what it looks like on my machine:
<br/>

<p align="left">
<img src=figs/ssh.png width="400">
</p>

## Login nodes

- Use for navigation, housekeeping, and job submission.
- Any process that is active for >20 minutes or uses >1GB will be killed.
- Good practice to avoid ever hitting these limits, since the login nodes can get clogged: get in the habit of doing all computations through a job, either interactive or non-interactive. 

</br>

- See OSC's page [Login environment at OSC](https://www.osc.edu/supercomputing/login-environment-at-osc).

## Filesystem I: long-term & backed-up

- **Home**
  - `$HOME` or `~`
  - 500 GB capacity, daily back-ups
  - Somewhat oddly, your home dir will include your first project ID, e.g. `/users/PAS0471/jelmer`. But this is not the project dir!


<br/>

- **Project**
  - `/fs/project/<projectID>` -- **MCIC project:** `/fs/project/PAS0471`
  - The MCIC project currently has limits of 50 TB and 10 million files.
  - File number limit can be more of a problem than total size!
  - Also backed up daily.

## Filesystem: scratch

- `/fs/scratch/<projectID>`, sometimes `fs/ess/scratch/<projectID>`
- MCIC project: `/fs/scratch/PAS0471`
- Create your own dir here:

```bash
$ mkdir /fs/scratch/PAS0471/<username>
```

<br/>

- Fast I/O, good to use for large input and output files
- Temporary: deleted after 120 days, and not backed up

## Filesystem: `$TMPDIR`

- Storage on compute nodes, 1 TB max
- Available in the job script through the environment variable `$TMPDIR`,
  no need to bother with the actual path.
- **Deleted after job ends** -- so copy to/from in job scripts!
  
```bash
cp -R $HOME/my/data/ $TMPDIR/
[...analyze data....]
cp -R $TMPDIR/* $HOME/my/data/
```

<br/>

- Or better yet, copy the data even if the jobs was killed:
```bash
trap "cd $PBS_O_WORKDIR;mkdir $PBS_JOBID;cp -R $TMPDIR/* $PBS_JOBID;exit" TERM
```

## Filesystem

- See [here](https://www.osc.edu/supercomputing/storage-environment-at-osc/available-file-systems) for more info about the filesystems

## Things to consider for compute jobs

- **Number of nodes and cores**
  - Only ask for >1 node when you have explicit paralellization in your script

<br/>

- **Memory**
  - Optional to specify, default is ~4 GB per core

<br/>

- **Walltime**
  - Job will get killed when it hits the limit!
  - Specifying the time can be a trade-off: shorter jobs are likely to start sooner


## Things to consider for compute jobs

- **Queue**
  - Default (no specification) will work in most cases.
  - For short and interactive jobs, consider the `debug` queue
  - There are separate queues for jobs requiring many nodes, jobs using GPUs, etc. ([list of queues at Owens](https://www.osc.edu/resources/technical_support/supercomputers/owens/queues_and_reservations) and [list of queues at Pitzer](https://www.osc.edu/supercomputing/computing/pitzer/queues_and_reservations)).
  - Max. walltime for most queues is 168 h (1 week).    
    For longer jobs, *request access* to the `longserial` queue on Pitzer.
  
<br/>

- **Project**
  - MCIC's project is `PAS0471`.
  - If you have only one project, you don't need to specify it.


## Transferring files

- For small/medium transfers (<1 Gb), you can `scp` or `rsync` to the login node:
```bash
$ scp /path/in/local/file.txt <user>@owens.osc.edu:/path/in/remote/
$ scp /path/in/local/file.txt jo:/path/in/remote/

$ rsync -avrz --progress /path/in/local/dir <user>@owens.osc.edu:/path/in/remote/
```

- For large transfers, use `sftp`:
```bash
$ sftp sftp.osc.edu
sftp> put /path/in/local/file.txt /path/in/remote
sftp> put file.txt    # If file is in present dir in local, and can go to $HOME in remote
sftp> get /path/in/remote/file.txt /path/in/local/
```

- For large transfers, **Globus** is also an option ([link](https://www.osc.edu/resources/getting_started/howto/howto_transfer_files_using_globus_connect))
  
## Pre-installed software

- For a list of installed software, see https://www.osc.edu/resources/available_software

- You can also check for software availability on the command line:
```bash
$ module spider [search-term]       # All installed software
$ module avail [search-term]        # Available software, given current environment
```

<br/>

- Loading software is also done with `module` commands:
```bash
$ module load R                     # Load default version
$ module load R/4.0.2-gnu9.1        # Load a specific version (better)
$ module unload R
```

## Other software

- Manually installing is possible, but can be tricky, since you have no admin (`sudo`) rights

- I can install software for you and make it available through `module`. In that case, you first need to make these non-standard modules available, e.g.:
```bash
$ module use /users/jelmer/osu5685/local/share/lmodfiles
```

- But, I am subject to same `sudo` restrictions when installing.

<br/>
 
- `conda` and `Singularity` containers can be alternatives when installation is tricky,
and are in fact superior for reproducibility purposes.

## `conda`

- `conda` is a software environment manager:
  - Install software *into a conda environment*, no need for `sudo` rights
  - Takes care of dependencies
  - Can easily have different environments with different version of the same software - load and unload similar to the `module` system
  - A lot of bioinformatics software is available through `conda`

## `conda` in practice

- Load the appropriate module at OSC: 
```bash
$ module load python/3.6-conda5.2
```

- Create a new environment with `MultiQC` installed:
```bash
$ conda create -y -n my-multiqc-env -c bioconda multiqc
```

- Activate the multiqc environment and see if we can run it:
```bash
[<user>@owens-login01 ~]$ conda activate my-multiqc-env
(multiqc) [<user>@owens-login01 ~]$ multiqc --help    # Note environment indicator
```

## Switch to `SLURM`

- `Pitzer` now runs the `SLURM` scheduler:

<p align="center">
<img src=figs/slurm.png width="800">
</p>

- `Owens` will also start running `SLURM` instead of `PBS`/`Torque`, probably at the end of 2020.
- At least, a "*compatibility mode*" is being run, where `PBS` jobs will be converted to `SLURM` jobs.
- For more info, see the [SLURM migration page](https://www.osc.edu/supercomputing/knowledge-base/slurm_migration)

## Interactive jobs on `SLURM`

- Start a 1-hour, single-core interactive job with a `bash` shell:
```sh
$ srun --nodes=1 --ntasks-per-node=1 --time=01:00:00 --pty bash -i
```

- Can also use `salloc` and `sinteractive`.

## Batch jobs on `SLURM`

- Unlike in `PBS`, job script needs to start with shebang line:
```sh
#!/bin/bash
```

- Submit a job:
```sh
$ sbatch myscript.sh [script-options] [script-args]
```

- Monitor your jobs:
```bash
$ squeue -u <username>
```

## Some common `sbatch` options:

```sh
#SBATCH --nodes=<N>
#SBATCH --ntasks-per-node=<N>
#SBATCH --time=HH:MM:SS
#SBATCH --mem=Xgb               # Default is Mb, hence "gb" specification
#SBATCH --job-name=<myjobname>
#SBATCH --account=<projectID>
```

## Some more advances `sbatch` options:

```sh
--begin=2020-10-20T12:00:00
```

## Job environment variables:
