<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>A Practical Introduction to OSC</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jelmer Poelstra" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# A Practical Introduction to OSC
### Jelmer Poelstra
### MCIC Wooster
### 2020/12/16 (updated: 2020-12-09)

---

class: inverse middle center



# Overview

.pull-left[
### [Introduction](#introduction)
### [Connecting to OSC](#connect)
### [Transferring Files](#transfer)
### [Software at OSC](#software)
]

---
class:inverse middle center
name: overview

# Overview

.pull-left[
### [Introduction](#introduction)
### [Connecting to OSC](#connect)
### [Transferring Files](#transfer)
### [Software at OSC](#software)
]

.pull-right[
### [Starting a Compute Job](#jobs)
### [Compute Job Options](#job_options)
### [Bonus Material](#bonus)
]

---
class: inverse middle center
name: introduction

# Introduction

-----

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

### [[Back to overview]](#overview)

---

## What is OSC?

- The **Ohio Supercomputer Center (OSC)** provides computing
  resources across the state of Ohio (it is not a part of OSU).

- Their main services are two supercomputers,   
  and the infrastructure for their usage.

--

&lt;br&gt;

- Research usage is charged but via institutions
  at [heavily subsidized rates](https://www.osc.edu/content/academic_fee_model_faq).

---

## Supercomputer?

- **A highly interconnected set of many processors and storage units.**

- Also known as:
  - A **cluster**
  - A High-Performance Computing cluster (**HPC cluster**)

&lt;br&gt;

--

### When do we need a supercomputer?

1. When our dataset is too large to be handled by our computer.

--

2. We have analyses that *could* run on our own computer,  
  but we save (a lot of) time with a cluster if:
  
  - We need to repeat a computation (or have many datasets).

  - We could distribute a single analysis across multiple computers.
  
---

## Learning about OSC

- OSC provides excellent introductory material at their   
  [Getting Started Page](https://www.osc.edu/resources/getting_started).
**Do read these!**

&lt;br&gt;

&lt;p align="center"&gt;
&lt;img src=figs_OSC/gettingstarted.png width="560"&gt;
&lt;/p&gt;

---

## Learning about OSC (cont.)

- Also have a look at all the ["HOWTO" pages](https://www.osc.edu/resources/getting_started/howto) --   
  includes more advanced material.

&lt;br&gt;

&lt;p align="center"&gt;
&lt;img src=figs_OSC/HOWTOs.png width="500"&gt;
&lt;/p&gt;

---

## Learning about OSC (cont.)

- OSC regularly has online introductory sessions,   
  both overviews and more hands-on sessions --
  see the [OSC Events page](https://www.osc.edu/events).

&lt;br/&gt;

- There is also a Carpentry-style tutorial available [here](https://khill42.github.io/OSC_IntroHPC).

---

## Terminology: cluster and node

#### Cluster

- Many processors and storage units tied together.

- OSC currently has two clusters: **_Pitzer_** and **_Owens_**.   
  Largely separate, but same long-term storage space is accessed by both.

&lt;br&gt;

--

#### Node

- Essentially a (large/powerful) computer -- one of many in a cluster.

- Most jobs run on a single node.   
  (Parallelization across nodes is possible but advanced.)

---

## Terminology: core

#### Core

- One of often many *processors* in a node --   
  e.g. standard *Pitzer* nodes have 40-48 cores.

--

- Each core can run and be reserved independently.

- But, using multiple cores for a job is also common, to:
  - Parallelize computations across cores
  - Access more memory

--

- **Note:** *SLURM* often uses the terms **CPU** and core interchangeably.

---

## Terminology: cluster &gt; node &gt; core

&lt;br&gt;

&lt;p align="center"&gt;
&lt;img src=figs_OSC/cluster-node-core.png width="100%"&gt;
&lt;/p&gt;

---

## Terminology (cont.)

#### Memory

- RAM (Random Access Memory), not to be confused with hard disk storage.

- Many programs, including R, load all the data that is used into memory.   

---
class:inverse middle center
name:connect

# Connecting to OSC

-----

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

### [[Back to overview]](#overview)

---

## Connecting to OSC with "OnDemand"

- You can make use of OSC not only through `ssh` at the command line,   
but also through the web browser: from https://ondemand.osc.edu.

&lt;p align="center"&gt;
&lt;img src=figs_OSC/ondemand1.png width="900"&gt;
&lt;/p&gt;

--

- At OnDemand, you can e.g.:

  - Browse and upload files
  - Submit and manage jobs visually
  - Access a terminal
  - Run RStudio Server

--

&lt;br&gt;

- **Interface Demo**

--

&lt;br&gt;

- See the bonus slides starting [here](#ssh) for logging in from your local terminal.


---

## Login nodes

- After logging in to OSC, you're on a **login node**.

&lt;p align="center"&gt;
&lt;img src=figs_OSC/cluster_diagram.png width="900"&gt;
&lt;/p&gt;


---

## Login nodes: best practices

- Use login nodes for *navigation*, *housekeeping*, and *job submission*.

- Any process that is active for &gt;20 minutes or uses &gt;1GB **will be killed**.

- It is good practice to avoid even getting close to these limits:   
  login nodes are *shared by everyone*, and can get clogged.   

&lt;br&gt;

--
 
- Fore more info, see OSC's page [Login environment at OSC](https://www.osc.edu/supercomputing/login-environment-at-osc).

---
class: inverse middle center
name: transfer

# Transferring Files

-----

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

### [[Back to overview]](#overview)

---

## Transferring files

There are several ways to transfer files between your local computer and OSC,
depending on your preferences and **the size of the transfer**.

---

## Transferring files: OnDemand and Globus

#### For small transfers (&lt;1 GB): use OnDemand

&lt;p align="center"&gt;
&lt;img src=figs_OSC/ondemand2_circle.png width="900"&gt;
&lt;/p&gt;

--

----

&lt;p align="center"&gt;
&lt;img src=figs_OSC/ondemand3_circle.png width="900"&gt;
&lt;/p&gt;

--

-----

&lt;br&gt;

#### For large transfers (&gt;1 GB): use Globus

- Especially useful for very large and/or complex transfers.

- Does need a [local installation](https://www.osc.edu/resources/getting_started/howto/howto_transfer_files_using_globus_connect) and some set-up.

---
background-color: #ededed

## Transferring files: using your **local** shell

#### For small transfers (&lt;1 GB): use `scp` / `rsync`

```bash
# scp
$ scp /path/in/local/file.txt &lt;user&gt;@owens.osc.edu:/path/in/remote/
```

```bash
# rsync (recommended, especially to keep folders synched)
$ rsync -avrz --progress /path/in/local/dir \
    &lt;user&gt;@owens.osc.edu:/path/in/remote/
```

-----

#### For any transfer size: use `sftp`

```bash
$ sftp sftp.osc.edu
sftp&gt; put /path/in/local/file.txt /path/in/remote

sftp&gt; put file.txt    # From local working dir to $HOME in remote

sftp&gt; get /path/in/remote/file.txt /path/in/local/
```


---
class: inverse middle center
name: software

# Software at OSC

-----

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

### [[Back to overview]](#overview)

---

## Available software at OSC
 
- For a list of software that has been installed at OSC,   
  and is available for all users, see:      
  &lt;https://www.osc.edu/resources/available_software&gt;.

---
background-color: #ededed

## Available software at OSC (cont.)

- Loading software is also done with `module` commands:
```bash
$ module load R               # Load default version
$ module load R/4.0.2-gnu9.1  # Load a specific version (better)
$ module unload R
```

&lt;br&gt;

-----

&lt;br&gt;

- You can also check for software availability on the command line:
```bash
$ module spider [search-term]  # All installed software
$ module avail [search-term]   # Available software,
                                   # given current environment
```


---

## What if software isn't installed at OSC?

### 1. Install it yourself

- Possible but can be tricky due to lack of admin rights.

--

### 2. Get others to install it

- For commonly used software, send an email to &lt;oschelp@osc.edu&gt;.

- I can help with (more niche) bioinformatics software.

--

### 3. Alternative approaches

- The **`conda`** software management system, and **`Singularity` containers**
  are alternatives when installation is tricky.

- These are also superior for reproducibility purposes.   
  For a `conda` example, see the bonus slides starting from [here](#conda).


---
class: inverse middle center
name: jobs

# Starting a Compute Job
# (i.e., Accessing Compute Nodes)

-----

&lt;br&gt;&lt;br&gt;&lt;br&gt;

### [[Back to overview]](#overview)

---

## Starting a compute job: background

- To do analyses at OSC, you need access to a **compute node**.
  
- **Automated scheduling software** allows 100s of people with different         requirements to access cluster effectively and fairly.

--

- Since Dec 15, 2020, both Pitzer and Owens use the **_SLURM_ scheduler**.

---

## Starting a compute job: how?

- In OnDemand, start an "**Interactive App**" like RStudio Server.   

- Provide *SLURM* directives in or along with **scripts**.
  
- Run an **interactive shell job**.

--

&lt;br&gt;

#### You'll always need to provide instructions to the *SLURM* scheduler to reserve the desired resources.


---

## Starting a compute job: OnDemand Apps

&lt;p align="center"&gt;
&lt;img src=figs_OSC/apps.png width="570"&gt;
&lt;/p&gt;

---
name:rstudio_server_job

## Starting a compute job: OnDemand Apps (cont.)

&lt;p align="center"&gt;
&lt;img src=figs_OSC/submit-RStudio.png width="400"&gt;
&lt;/p&gt;

&lt;p align="center"&gt;
&lt;img src=figs_OSC/submit-RStudio2.png width="400"&gt;
&lt;/p&gt;


---
background-color: #ededed

## Starting a compute job: Shell script

- To submit a job:

  ```sh
  $ sbatch my_script_name.sh
  ```

&lt;br&gt;

- The `sbatch` instructions go to the top of your script:

  ```sh
  #!/bin/bash
  
  #SBATCH --account=PAS0471
  #SBATCH --time=00:45:00
  #SBATCH --nodes=1-1
  #SBATCH --cpus-per-task=2
  #SBATCH --mem=8G
  ```

---
background-color: #ededed

## Starting a compute job: Interactive job

- To start a 30-minute, single-core job:

  ```sh
  sinteractive
  ```

- This will provide you interactive shell access on a compute node. 

---
class: inverse middle center
name: job_options

# Compute Job Options

-----

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

### [[Back to overview]](#overview)

---

## Compute job options: Nodes and cores

- Only ask for &gt;1 **node** when you have explicit parallelization   
  (*you probably don't*).

- R will not even use multiple **cores** by default,   
  though some of the packages we'll work with *can* do this.


&lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt;

```sh
#SBATCH --nodes=1-1             # 1 node min. and max. 

# Two ways to ask for two cores:

#SBATCH --cpus-per-task=2       # Preferred for multi-threading
#SBATCH --ntasks-per-node=2     # Preferred for multi-process
```

---

## Compute job options: Memory

- If you request more memory than that available on a single core,   
  **you will automatically be assigned multiple cores**.

--

&lt;br&gt;

- **If your job hits the memory limit, it may be killed!**
  
  I.e., a process will not be automatically prevented from
  using too much memory.

--

  - Many *programs* will allow you to specify maximum memory usage.
  
  - But your jobs may have inherent minimum memory requirements
    (e.g. When your job involves loading a 10 GB file into memory),
    so make sure to **ask for sufficient memory**.

&lt;br&gt; &lt;br&gt; &lt;br&gt;

--

```sh
#SBATCH --mem=20G       # Default unit is MB; use "G" for GB
```

---

## Compute job options: Time

- You need to specify a **time limit** for your job (= "wall time").

- Your job **gets killed** as soon as it hits the time limit!

--

- Specifying time can be a trade-off: shorter jobs are likely to start sooner.   
Still, **ask for (much) more time than you think you will need**.
  - Better safe than sorry.
  - Your project will be charged for the time *actually used*.

--

&lt;br&gt; &lt;br&gt;

```sh
# Acceptable time formats include "minutes", "minutes:seconds",
# "hours:minutes:seconds","days-hours", "days-hours:minutes" and
# "days-hours:minutes:seconds".          

#SBATCH --time=60            # 1 hour

#SBATCH --time=03:30:00      # 3.5 hours

#SBATCH --time=5-0           # 5 days
```


---

## Compute job options: Project

- Your jobs always needs to be billed to a project.   
  (If you have only one project, no need to specify it.)

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

```sh
#SBATCH --account=PAS0471
```

---

## Compute job options: Queue

There are several "queues" that jobs can be submitted to.

- **The default queue (no specification) will work in most cases.**

- There are separate queues for jobs that require many nodes,
  jobs that use *GPUs*, etc.: [list of queues at Owens](https://www.osc.edu/resources/technical_support/supercomputers/owens/queues_and_reservations) and [list of queues at Pitzer](https://www.osc.edu/supercomputing/computing/pitzer/queues_and_reservations).

--

- **The maximum walltime** for most queues is 168 h (1 week).   
  For longer jobs, *request access* to the `longserial` queue on Pitzer.
  
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

--

```sh
#SBATCH --partition=longserial
```

---
class: center middle inverse

# Questions?

-----


---
class: inverse center middle
name:bonus

# Bonus Materials

-----

### I: [OSC File Systems](#filesystems)
### II: [Accessing OSC In Your Shell (`ssh`)](#shell)
### III: [Monitoring *SLURM* Jobs](#monitor)
### IV: [`conda`](#conda)

&lt;br&gt;&lt;br&gt;

### [[Back to overview]](#overview)

---
class: inverse center middle
name:filesystems

# Bonus Material I:
# OSC File Systems

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

### [[Back to overview]](#overview)

---
background-color: #ededed

## OSC file systems

- OSC has several file systems with different typical use cases:
  we'll discuss in terms of short-term versus long-terms storage. 

---
background-color: #ededed

## File systems: long-term storage locations

### Home dir

- Can always be accessed with shortcuts **`$HOME`** and **`~`**.

- 500 GB capacity, daily back-ups.

- Your home dir will include your first project ID, e.g. `/users/PAS0471/jelmer`.
  (However, this is not the project dir!)

--

### Project dir
  
- `/fs/project/&lt;projectID&gt;`. (MCIC project: `/fs/project/PAS0471`)
  
- The total storage limit is whatever a PI sets this too.   
    Charges are for the reserved space, not actual usage.
  
- Daily back-ups.

---
background-color: #ededed

## File systems: short-term storage locations

### scratch

- `/fs/scratch/&lt;projectID&gt;`, sometimes `fs/ess/scratch/&lt;projectID&gt;`.

- Fast I/O, good to use for large input and output files.

- Temporary: deleted after 120 days, and not backed up.

--

### `$TMPDIR`

- Represents storage space *on your compute node*. 1 TB max.

- Available in the job script through the environment variable `$TMPDIR`.

- **Deleted after job ends** -- so copy to/from in job scripts!

- See [this bonus slide](#tmpdir) for some example code for working with `$TMPDIR`.

---
name: tmpdir
background-color: #ededed

## Working with `$TMPDIR` in scripts

#### Copy input to $TMPDIR, copy output to home:

```bash
cp -R $HOME/my/data/ $TMPDIR/
#[...analyze data with I/O happening in $TMPDIR....]
cp -R $TMPDIR/* $HOME/my/data/
```

&lt;br&gt;

#### This trick will copy the data even if the job is killed:

```bash
trap "cd $PBS_O_WORKDIR;mkdir $PBS_JOBID;cp -R $TMPDIR/* $PBS_JOBID;exit" TERM
```

---
background-color: #ededed

## File systems: in closing

&lt;br&gt;

#### If this was a bit much:

For those working with "small" NGS datasets
(like from MiSeq runs),   
you may be able to **stick to your home dir** both for storage and computing.

&lt;br&gt;

#### See [here](https://www.osc.edu/supercomputing/storage-environment-at-osc/available-file-systems) for more info about the file systems.

---
class: inverse center middle
name: shell

# Bonus Material II:
# Accessing OSC In Your Shell (`ssh`)

-----

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

### [[Back to overview]](#overview)

---
background-color: #ededed

## Connecting to OSC with `ssh`

- If you want to log in from your *own local terminal*,   
  you need to **connect through `ssh`.**
  
- Basic `ssh` usage:
  
  ```sh
  $ ssh &lt;username&gt;@pitzer.osc.edu  # e.g. jelmer@pitzer.osc.edu     
  $ ssh &lt;username&gt;@owens.osc.edu   # e.g. jelmer@ownes.osc.edu     
  ```
  
  - You will be prompted for your OSC password.

---
background-color: #ededed

## `ssh` set-up: avoid being prompted for password

1. On your own computer, generate a key:
```bash
$ ssh-keygen -t rsa
```

1. On your own computer, transfer the key to the remote computer:
```bash
$ cat ~/.ssh/id_rsa.pub | ssh &lt;user&gt;@owens.osc.edu 'mkdir -p .ssh &amp;&amp; cat &gt;&gt; .ssh/authorized_keys'
```

1. On remote computer (OSC), set permissions:
```bash
$ chmod 700 .ssh; chmod 640 .ssh/authorized_keys
```

&lt;br&gt;

- For more details, see this [Tecmint post](https://www.tecmint.com/ssh-passwordless-login-using-ssh-keygen-in-5-easy-steps/).

---
background-color: #ededed

## `ssh` set-up: use shortcuts

1. Create a file called `~/.ssh/config`:
```bash
$ touch `~/.ssh/config`
```

1. Open the file and add your alias(es):
```bash
&gt; Host &lt;arbitrary-alias-name&gt;    
&gt;     HostName &lt;remote-name&gt;
&gt;     User &lt;user-name&gt;
```

--

&lt;br&gt;

- This is what it looks like on my machine, so I can login using "`ssh jo`":
&lt;br/&gt;

&lt;p align="center"&gt;
&lt;img src=figs_OSC/ssh.png width="400"&gt;
&lt;/p&gt;


---
name: monitor
class: inverse center middle

# Bonus Material III:
# Monitoring *SLURM* Jobs

-----

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

### [[Back to overview]](#overview)

---
background-color: #ededed

## Monitoring *SLURM* Jobs


---
name: conda
class: inverse center middle

# Bonus Material IV:
# `conda`

-----

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

### [[Back to overview]](#overview)

---
background-color: #ededed

## `conda`

**`conda` is a software environment manager**
  
- Install software *into a conda environment*, no need for admin (`sudo`) rights
  
- Takes care of software dependencies.
  
- You can easily have different "environments" that each have a different version
  of the same software - load and unload similar to the `module` system.
  
- A *lot* of bioinformatics software is available through `conda`.

---
background-color: #ededed

## `conda` in practice

- Load the `conda` module at OSC (part of Python): 

  ```bash
  $ module load python/3.6-conda5.2
  ```

&lt;br&gt;

- Create a new environment with "`multiqc`" installed:

  ```bash
  $ conda create -y -n my-multiqc-env -c bioconda multiqc
  ```

&lt;br&gt;

- Activate the `multiqc` environment and see if we can run it:

  ```bash
  [&lt;user&gt;@owens-login01 ~]$ conda activate my-multiqc-env

  # Note environment indicator
  (multiqc) [&lt;user&gt;@owens-login01 ~]$ multiqc --help
  ```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
