---
title: "<br>Workflow part II:<br>ASV Infererence and Taxon Assignment"
output:
  rmarkdown::html_document:
    code_download: true
    theme: cerulean
    toc: true
    toc_float: true
    css: my.css
---

```{r knitr_options, echo=FALSE}
knitr::opts_chunk$set(results='hide',
                      eval = FALSE,
                      class.source="r_code",
                      class.output="r_output",
                      class.warning="r_warning",
                      class.message="r_warning",
                      class.error="r_error")
```

<br>

-----

## Goals

Process metabarcoding reads, post-adapter removal, including:

- Evaluation of quality
- Quality trimming and filtering
- Error correction and denoising
- Read merging
- Assigning taxonomy

-----

## Notes

- This documented was adapted from [Callahan et al. 2006](https://f1000research.com/articles/5-1492/v2)
  by Matthew Willman, with further edits by Soledad Benitez Ponce and Jelmer Poelstra."
  
- You can download this `Rmd` (R Markdown) file by clicking the `Code` button in
  the top-right of this page.
  To convert an `Rmd` file to an R script,
  type the following in an R console: `knitr::purl(input="<filename>.Rmd")`.


-----

## Step 1: Setup

### Install and load packages

```{r}
.cran_packages <- c("ggplot2", "gridExtra")
.bioc_packages <- c("dada2", "phyloseq", "DECIPHER", "phangorn")

.inst <- .cran_packages %in% installed.packages()
if(any(!.inst)) {
   install.packages(.cran_packages[!.inst])
}

.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)) {
   BiocManager::install(.bioc_packages[!.inst])
}

sapply(c(.cran_packages, .bioc_packages), require, character.only = TRUE)

```

### Set the file paths

```{r}
input <- "/Users/benitezponce.1/Box/HCC_AmpSeq_subset/V4/fastq/cutadapt_out/16S/second_trim/"
output<- "for_dada2/dada2_out/"
tax_key<- "for_dada2/silva_nr99_v138_train_set.fa" #check up to date version https://benjjneb.github.io/dada2/training.html
sample_names<- "for_dada2/sample_meta.txt"
dir.create(output, recursive=T)
```

### Assign files to forward and reverse reads 

For forward and reverse fastq filenames which have the following name format: `<SAMPLENAME>_R1_001.fastq` and `<SAMPLENAME>_R2_001.fastq`.

```{r}
fnFs <- sort(list.files(input, pattern="_R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(input, pattern="_R2_001.fastq.gz", full.names = TRUE))
```

### Extract sample names

Extract the sample names from the `.fastq` file names, assuming file names have format: `<SAMPLENAME>_XXX.fastq`.

The resulting `sample.names` should match the SampleID column in the `sample_data_example` file.

```{r}
sample.names <- sapply(strsplit(basename(fnFs), "-V4-V5_"), `[`, 1)
```

-----

## Step 2: QC

### Plot sequence quality data

You can generate and evaluate plots individually:

```{r}
plotQualityProfile(fnFs[1:2])
```

<br>

Or generate pdf files to inspect multiple plots at once:

```{r}
ii <- sample(length(fnFs), 3)
pdf("Error_profiles.pdf")
par(mfrow=c(2,3))
for (i in ii) { print (plotQualityProfile(fnFs[i]) + ggtitle("Fwd")) }
ii <- sample(length(fnRs), 3)
for (i in ii) { print (plotQualityProfile(fnRs[i]) + ggtitle("Rev")) }
dev.off()
```




-----

## Step 3: Trimming

The `truncLen` parameters should be based on sequence quality visualized above.
It is suggested to trim first 10 nucleotides of each read,
empirical observations across Illumina datasets show these positions are likely to contain pathological errors. Trimming length could be different for R1 and R2,
but ideally not too drastically so, in order to maximize merging pairs in subsequent steps.

First, define paths for trimming:

```{r}
filtFs <- file.path(output, "filtered", paste0(sample.names, "_F_filt.fastq"))
filtRs <- file.path(output, "filtered", paste0(sample.names, "_R_filt.fastq"))
```

<br>

Then, perform the trimming:

```{r}
print("filter and Trimming")
Sys.time()
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft=10, truncLen=c(250,210), maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=F, compress=FALSE, multithread=TRUE, verbose=TRUE) 
print("done")
Sys.time()
```


-----

## Step 4: Dereplication & Error Training

```{r}
derepFs <- derepFastq(filtFs, verbose=F)
derepRs <- derepFastq(filtRs, verbose=F)

print("learning Errors")
Sys.time()
errF<- learnErrors(derepFs, multithread=T, verbose=T)
errR<- learnErrors(derepRs, multithread=T, verbose=T)
print("done")
Sys.time()

names(derepFs) <- sample.names
names(derepRs) <- sample.names
```

<br>

Plot errors to verify that error rates have been reasonable well-estimated.
Observe the fit between observed error rates (points) and fitted error rates (lines):

```{r}
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
```

-----

## Step 5: Infer ASVs

We will run the dada algorithm to infer ASVs.

This step is more computationally intensity. Indepedant inference by sample (pool=F) will keep memory requirements linear to sample number. Pooling will increase computation time but improve detection of rare variants seen once or twice in an individual sample but many times across all samples.

```{r}
print("inferring ASVs (running dada algorithm)")
Sys.time()
dadaFs <- dada(derepFs, err=errF, multithread=T, pool=F)
dadaRs <- dada(derepRs, err=errR, multithread=T, pool=F)
print("done")
Sys.time()

#inspect object
dadaFs[[1]]
```

-----

## Step 6: Merge Read Pairs

In this step, we will merge read pairs and generate sequence table of paired ASVs.

```{r}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)

seqtab.all <- makeSequenceTable(mergers)
dim(seqtab.all)
```

-----

## Step 7: Summarize Sequences and Remove Chimeras

```{r}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab.all)))

#seqtab2 <- seqtab[,nchar(colnames(seqtab.all)) %in% seq(250,256)] #If need to remove sequences of a particular length (e.g. too long)

#remove chimeras 
seqtab <- removeBimeraDenovo(seqtab.all, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab)

#proportion of removed sequences
sum(seqtab)/sum(seqtab.all)

#save seqtab file as rds object

saveRDS(seqtab, file="seqtab_V4.rds") 
```


-----


## Step 8: Generate a Summary Table

In this step, we will generate a summary table of the number of sequences
processed and outputs of different steps of the pipeline.

This information is generally used to further evaluate characteristics and
quality of the run, sample-to-sample variation,
and resulting sequencing depth for each sample. 

```{r}
getN <- function(x) sum(getUniques(x))

track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab))

# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)

write.table(track, file="track.txt")
```

-----

## Step 9: Assign Taxonomy to ASVs

Depending on the marker gene and the data, you will choose the appropriate file for this step. Several files have been formatted for taxony assignments in DADA2 pipeline and are available in the following website: https://benjjneb.github.io/dada2/index.html 

```{r}
print("Assigning taxa to ASVs")
Sys.time()
taxa <- assignTaxonomy(seqtab, tax_key, multithread=TRUE)
colnames(taxa) <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus")
print("done")
Sys.time()
```

-----

## Step 10: Phylogenetic Tree Estimation

A phylogenetic tree can be estimated for the sequence data you generated. Depending on the number of ASVs recovered and the phylogenetic tree algorithm of choice, this step could take several days. Simpler trees will be less computationally intensive. Depending on the marker gene you are working on, you might choose or not to use this option. 

```{r, eval=FALSE}
#seqtab<- readRDS("seqtab_V4.rds")
seqs <- getSequences(seqtab)
names(seqs) <- seqs # This propagates to the tip labels of the tree. At this stage ASV labels are full ASV sequence
alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA, iterations = 5, refinements = 5)

print("Computing pairwise distances from ASVs")
Sys.time()
phang.align <- phyDat(as(alignment, "matrix"), type="DNA")
dm <- dist.ml(phang.align)
treeNJ <- NJ(dm) # Note, tip order != sequence order
fit = pml(treeNJ, data=phang.align)
print("done")
Sys.time()

fitGTR <- update(fit, k=4, inv=0.2)
print("done")
Sys.time()

print("Computing likelihood of tree")
Sys.time()
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                      rearrangement = "stochastic", control = pml.control(trace = 0))
print("done")
Sys.time()
```



-----

## Step 11: Generate Output Files

In this step, we will generate output files formatted for downstream analysis in phyloseq

Data will be extracted from the DADA2 outputs and formatted for downstream analysis and data inspection. The last step of this pipeline saves a phyloseq object as an .RDS file and can be imported directly in phyloseq. 

```{r}
#rename ASV names 
asv_seqs<- colnames(seqtab)
asv_headers<- vector(dim(seqtab)[2], mode="character")

for (i in 1:dim(seqtab)[2]) {
	asv_headers[i] <- paste(">ASV", i, sep="_")
}

#write ASV fasta for final ASV seqs. This fasta file can also be used for phylogenetic tree inference in different platforms
asv_fasta<- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "ASVs.fa")

#count table
asv_tab <- t(seqtab)
row.names(asv_tab) <- sub(">", "", asv_headers)
write.table(asv_tab, "ASVs_counts.tsv", sep="\t", quote=F, col.names=NA)

#tax table
asv_tax <- taxa
row.names(asv_tax) <- sub(">", "", asv_headers)
write.table(asv_tax, "ASVs_taxonomy.tsv", sep="\t", quote=F, col.names=NA)

count_tab<- read.table("ASVs_counts.tsv", header=T, row.names=1, check.names=F, sep="\t")
tax_tab<- as.matrix(read.table("ASVs_taxonomy.tsv", header=T, row.names=1, check.names=F, sep="\t"))

#Load and prepare sample data (prepared separately)
samdf <- read.table(file=sample_names, sep=("\t"), header=TRUE)
colnames(samdf)[1]<- "SampleID"
rownames(samdf)<- samdf$SampleID

ps <- phyloseq(otu_table(count_tab, taxa_are_rows=T), sample_data(samdf), tax_table(tax_tab), phy_tree(fitGTR$tree))

saveRDS(ps, file="ps_V4.rds")

print("done")
```

-----

## Resources

- [Callahan et al. 2006: "Bioconductor Workflow for Microbiome Data Analysis: from raw reads to community analyses"](https://f1000research.com/articles/5-1492/v2)
- [`dada2` documentation and tutorials](https://benjjneb.github.io/dada2/index.html )
- [Taxonomic references for `dada2`](https://benjjneb.github.io/dada2/training.html)
- [`cutadapt` documentation and tutorials](https://cutadapt.readthedocs.io/en/stable/index.html)


-----

## Bonus: Submit script as an OSC job

TODO
