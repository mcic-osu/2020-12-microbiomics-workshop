---
title: "A Practical Introduction to OSC"
author: "Jelmer Poelstra"
institute: "MCIC Wooster"
date: "2020/12/16 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

## What is OSC?

- The Ohio Supercomputer Center (OSC) provides computing
  resources across the state of Ohio (it is not a part of OSU).

- Their main services are two supercomputers
  and the infrastructure for their usage.

--

<br>

- Research usage is charged but via their institutions
  and at [heavily subsidized rates](https://www.osc.edu/content/academic_fee_model_faq)
  (education usage if entirely free).

---

## Supercomputer?

A supercomputer, AKA a (compute) **cluster**,   
is simply a connected set of many processors and storage units.

<br>

**When do we need a supercomputer?**

--

- When our dataset is too large to be handled by our computer.

--

- Or, we may have analyses that *could* run on our own computer,  
  but we get a massive speed gain with a cluster if:
  
  - We need to repeat a computation many times   
    (or are analyzing many datasets at the same time).

  - We could distribute a single analysis across multiple computers.
  
---

## Getting started at OSC

- OSC provides excellent introductory material at their [Getting Started Page](https://www.osc.edu/resources/getting_started).
**Do read these!**

<p align="center">
<img src=figs/gettingstarted.png width="500">
</p>

--

- Have a look at all the ["HOWTO" pages](https://www.osc.edu/resources/getting_started/howto) that includes more advanced material, too.

---

## Getting started at OSC (cont.)

- OSC regularly has online introductory sessions,   
  both overviews and more hands-on sessions --
  see the [OSC Events page](https://www.osc.edu/events).

<br/>

- There is also a Carpentry-style tutorial available [here](https://khill42.github.io/OSC_IntroHPC).

---

## Terminology

#### Cluster

Many processors and storage units tied together.

OSC has two clusters (*Pitzer* and *Owens*) and they are largely separate --   
you log in to them separately.
But the same long-term storage space is accessed by both.

<br>

--

#### Node

Essentially a (large) computer.

---

## Terminology (cont.)

#### Core

A processor within a node.
Standard *Pitzer* nodes, for instance, have 40-48 cores.
Each core can run and be reserved independently:
48 people may each be running a "job" on one core simultaneously.

(Note: While a **CPU** technically often has a collection of cores,
OSC's resource reservation system (*SLURM*) generally use the terms CPU and core interchangeably.)

--

<br>

#### Memory

RAM (Random Access Memory), not to be confused with hard disk storage.

Many programs, including R, load all the data that is used into memory.   
This makes it hard to work with multi-GB datasets on a personal computer.
Standard *Pitzer* nodes have 192 GB memory.

---

## Clusters at OSC

<p align="center">
<img src=figs/computers2.png width="500">
</p>

- **The "annotated" changes have happened just in the past few months:**

  - The *Ruby* cluster has been decommissioned

  - The *Pitzer* cluster just got an expansion


---

## Connecting to OSC with "OnDemand"

- You can make use of OSC not only through `ssh` at the command line,   
but also through the web browser: from https://ondemand.osc.edu.

<br>

<p align="center">
<img src=figs/ondemand1.png width="900">
</p>

<br>

--

- Here you can:
  - Browse and upload files
  - Submit and manage jobs visually,
  - Access a terminal
  - Start RStudio Server
  - Etc.

--

<br>

- **Interface Demo**

---
class:inverse

## Connecting to OSC with `ssh`

- If you'll be working at OSC a lot,
  especially if you need to submit jobs that run scripts,
  you will want to use the command line as well.
  
- If you want to log in from your *own local terminal*,   
  you need to **connect through `ssh`.**
  
- Basic `ssh` usage (note: you log in to a specific cluster, Pitzer or Owens):
```bash
$ ssh <username>@pitzer.osc.edu    # e.g. jelmer@owens.oscu.edu     
$ ssh <username>@owens.osc.edu
```
  
  - You will be prompted for your OSC password.

<br>

- For tips on logging in without using your password,
  and with name shortcuts, see the bonus slides starting [here](#ssh).

---


## Transferring files

There are several ways to transfer files between your local computer and OSC,
depending on your preferences and **the size of the transfer**.

---

## Transferring files: with GUIs

### For small transfers (<1 GB): use OnDemand

<p align="center">
<img src=figs/ondemand2_circle.png width="900">
</p>

--

----

<p align="center">
<img src=figs/ondemand3_circle.png width="900">
</p>

--

-----

### For large transfers (>1 GB): use Globus

- Especially useful for very large and/or complex transfers.

- Does need a [local installation](https://www.osc.edu/resources/getting_started/howto/howto_transfer_files_using_globus_connect) and some set-up.

---
class:inverse

## Transferring files: at the command line

### For small transfers (<1 GB): use `scp` / `rsync`

```bash
# scp
$ scp /path/in/local/file.txt <user>@owens.osc.edu:/path/in/remote/
```

```bash
# rsync (recommended, especially to keep folders synched)
$ rsync -avrz --progress /path/in/local/dir \
    <user>@owens.osc.edu:/path/in/remote/
```

--

-----

### For any transfer (>1 GB): use `sftp`

```bash
$ sftp sftp.osc.edu
sftp> put /path/in/local/file.txt /path/in/remote

sftp> put file.txt    # From local working dir to $HOME in remote

sftp> get /path/in/remote/file.txt /path/in/local/
```

---

## Login nodes

- After logging in to OSC, you're on a **login node**.

<p align="center">
<img src=figs/cluster_diagram.png width="900">
</p>


---

## Login nodes

- After logging in to OSC, you're on a **login node**.

  - This is true when you log in through `ssh` and also at OnDemand.
  
  - (Starting an App like RStudio Server connects you to a compute node.)

--

<br>

-----

- Use login nodes for *navigation*, *housekeeping*, and *job submission*.

- Any process that is active for >20 minutes or uses >1GB **will be killed**.

- It is good practice to avoid even getting close to these limits:   
  login nodes are *shared by everyone*, and can get clogged.   

- Fore more info, see OSC's page [Login environment at OSC](https://www.osc.edu/supercomputing/login-environment-at-osc).

--

<br>

-----

- All analyses should be done on **compute nodes**.

---

## Running "jobs" at OSC

- When you actually want to run some analysis at OSC,
 you will need to use a **compute node** rather than a *login node*.

- What to do when 100s of people, all with different access rights and needs
  (time, memory, number of cores, GPU or not),
  need to access nodes on a cluster?
  
  We need an automated scheduling mechanism is needed to distribute and manage
  these jobs.

--

<br>

-----

- There are several programs that can do this.
  This year, OSC gradually switched from one scheduler (PBS/Torque) to another (SLURM),
  and as of yesterday (Dec 15, 2020), **both Pitzer and Owens run with SLURM**.

- We will briefly demonstrate submitting a SLURM job in the shell session.

---

## How to submit jobs?

For example, you can:

1. In OnDemand, start an RStudio Server (or other interactive app).

2. In OnDemand, Use the Job Composer (largely visual interface to 3.)

3. In your shell, submit a script.

--

<br>

- **OnDemand Demo.**
- ADD SCREENSHOT OF INTERACTIVE JOB MENU?

--

<br>

-----

For all of these options,
**you have to provide special instructions to the SLURM scheduler.**

---

## Specifying job options: RStudio in OnDemand

<p align="center">
<img src=figs/submit-RStudio.png width="500">
</p>

---
class:inverse

## Specifying job options: in a shell script


```bash
#SBATCH --account=PAS0471
#SBATCH --time=00:45:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --mem-per-cpu=8G
```

---

## Things to consider for compute jobs

### Number of nodes and cores

- Only ask for >1 node when you have explicit parallelization   
  (you probably don't).

- **R will not even use multiple cores by default**,   
  though some of the packages we'll work with *can* do this.

- Asking for multiple cores can also increase the memory you have available
  (memory is shared across cores).

--

<br>

-----

### Memory

- Optional to specify, default is ~4 GB per core
- **ADD MORE INFO**

---

## Things to consider for compute jobs (cont.)

### Walltime

- You need to specify a **time limit** for your job.   
  (This is the "walltime" -- cf. "core-hours".)

- **Your job gets killed as soon as it hits the time limit!**

--

- Specifying the time can be a slight **trade-off**:   
  shorter jobs are likely to start sooner.
  
- But: better safe than sorry -- **ask for (much) more time than you think you will need**. (Your project will be charged for the time *actually used*.)

--

<br>

-----

### Project

- Your jobs always needs to be billed to a project.   
  (If you have only one project, no need to specify it.)

---

### Queue

There are several "queues" that jobs can be submitted to.

- **The default queue (no specification) will work in most cases.**

- For short and interactive jobs, consider the `debug` queue.

- There are separate queues for jobs that require many nodes,
  jobs that use *GPUs*, etc. ([list of queues at Owens](https://www.osc.edu/resources/technical_support/supercomputers/owens/queues_and_reservations) and [list of queues at Pitzer](https://www.osc.edu/supercomputing/computing/pitzer/queues_and_reservations)).

- **The maximum walltime** for most queues is 168 h (1 week).   
  For longer jobs, *request access* to the `longserial` queue on Pitzer.

---

## Software at OSC
 
- For a list of software that has been installed at OSC,   
  and is available for all users,   
  see https://www.osc.edu/resources/available_software.

---
class: inverse

## Software at OSC (cont.)

- You can also check for software availability on the command line:
```bash
$ module spider [search-term]       # All installed software
$ module avail [search-term]        # Available software, given current environment
```

<br/>

-----

- Loading software is also done with `module` commands:
```bash
$ module load R                     # Load default version
$ module load R/4.0.2-gnu9.1        # Load a specific version (better)
$ module unload R
```

---

## What if software isn't installed at OSC?

- **Manually installing software is possible but can be tricky**,
  since you and I have no admin rights (but R packages are usually fine!).

--

<br>

----

- For commonly used software, send an email to <oschelp@osc.edu>,
  and they may install it centrally for you.

- For bioinformatics software, including more niche stuff,
  I can also try to help.

--

<br>

------

- The **`conda`** software management system, and **`Singularity` containers**
  are alternatives when installation is tricky.

- These are also superior for reproducibility purposes.   
  For a `conda` example, see the bonus slides starting from [here](#conda).

---
class: inverse center middle

# Bonus Material

---

## Filesystems

- OSC has several filesystems with different typical use cases:
  we'll discuss in terms of short-term versus long-terms storage. 

---

## Filesystem I: long-term storage locations

### Home dir

- Can always be accessed with shortcuts **`$HOME`** and **`~`**.

- 500 GB capacity, daily back-ups.

- Your home dir will include your first project ID, e.g. `/users/PAS0471/jelmer`.
  (However, this is not the project dir!)

--

### Project dir
  
- `/fs/project/<projectID>`. (MCIC project: `/fs/project/PAS0471`)
  
- The total storage limit is whatever a PI sets this too.   
    Charges are for the reserved space, not actual usage.
  
- Daily back-ups.

---

## Filesystem II: short-term storage locations

### scratch

- `/fs/scratch/<projectID>`, sometimes `fs/ess/scratch/<projectID>`.

- Fast I/O, good to use for large input and output files.

- Temporary: deleted after 120 days, and not backed up.

--

### `$TMPDIR`

- Represents storage space *on your compute node*. 1 TB max.

- Available in the job script through the environment variable `$TMPDIR`.

- **Deleted after job ends** -- so copy to/from in job scripts!

- See [this bonus slide](#tmpdir) for some example code for working with `$TMPDIR`.

---
name: tmpdir

## Working with `$TMPDIR` in scripts

```bash
cp -R $HOME/my/data/ $TMPDIR/
[...analyze data....]
cp -R $TMPDIR/* $HOME/my/data/
```

<br/>

Or better yet, copy the data even if the jobs was killed:

```bash
trap "cd $PBS_O_WORKDIR;mkdir $PBS_JOBID;cp -R $TMPDIR/* $PBS_JOBID;exit" TERM
```

---

## File systems: in closing

- If this is all a bit much -- for those working with "small" NGS datasets
  (like from MiSeq runs), you may be able to **stick to your home dir**.

<br>

- See [here](https://www.osc.edu/supercomputing/storage-environment-at-osc/available-file-systems) for more info about the file systems.

---
name: ssh

## `ssh` set-up: avoid being prompted for password

- On your own computer, generate a key:
```bash
$ ssh-keygen -t rsa
```

- On your own computer, transfer the key to the remote computer:
```bash
$ cat ~/.ssh/id_rsa.pub | ssh <user>@owens.osc.edu 'mkdir -p .ssh && cat >> .ssh/authorized_keys'
```

- On remote computer (OSC), set permissions:
```bash
$ chmod 700 .ssh; chmod 640 .ssh/authorized_keys
```

<br>

- For more details, see this [Tecmint post](https://www.tecmint.com/ssh-passwordless-login-using-ssh-keygen-in-5-easy-steps/).

---

## `ssh` set-up: use shortcuts

- Create a file called `~/.ssh/config`:
```bash
$ touch `~/.ssh/config`
```

- Open the file and add your alias(es):
```bash
> Host <arbitrary-alias-name>    
>     HostName <remote-name>
>     User <user-name>
```

--

- This is what it looks like on my machine, so I can login using "`ssh jo`":
<br/>

<p align="center">
<img src=figs/ssh.png width="400">
</p>


---
name: conda

## `conda`

**`conda` is a software environment manager**
  
- Install software *into a conda environment*, no need for admin (`sudo`) rights
  
- Takes care of software dependencies.
  
- You can easily have different "environments" that each have a different version
  of the same software - load and unload similar to the `module` system.
  
- A *lot* of bioinformatics software is available through `conda`.

---

## `conda` in practice

- Load the `conda` module at OSC (part of Python): 

  ```bash
  $ module load python/3.6-conda5.2
  ```

<br>

- Create a new environment with "`multiqc`" installed:

  ```bash
  $ conda create -y -n my-multiqc-env -c bioconda multiqc
  ```

<br>

- Activate the `multiqc` environment and see if we can run it:

  ```bash
  [<user>@owens-login01 ~]$ conda activate my-multiqc-env

  # Note environment indicator
  (multiqc) [<user>@owens-login01 ~]$ multiqc --help
  ```
