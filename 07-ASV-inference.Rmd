---
title: "<br>Workflow part II:<br>ASV Inference and Taxon Assignment"
output:
  rmarkdown::html_document:
    code_download: true
    theme: cerulean
    toc: true
    toc_float: true
    css: my.css
---

```{r knitr_options, echo=FALSE}
knitr::opts_chunk$set(results='hide',
                      eval = FALSE,
                      #knitr::opts_knit$set(root.dir = "/fs/project/PAS0471/workshops/2020-12_micro/$USER"),
                      class.source="r_code",
                      class.output="r_output",
                      class.warning="r_warning",
                      class.message="r_warning",
                      class.error="r_error")
```

<br>

-----

## Goals

Process metabarcoding reads, post-adapter removal, including:

- Evaluation of quality
- Quality trimming and filtering
- Error correction and denoising
- Read merging
- Assigning taxonomy

-----

## Before We Get Started

### Notes

- This documented was adapted from [Callahan et al. 2006](https://f1000research.com/articles/5-1492/v2)
  by Matthew Willman, with further edits by Soledad Benitez Ponce and Jelmer Poelstra.
  
- To convert an `Rmd` (R Markdown) file to an R script,
  type the following in an R console:
  `knitr::purl(input="<filename>.Rmd")`.
  (You can download this `Rmd` file by clicking the `Code` button
  in the top-right of this page -- but we will open it at OSC.)

### Start an RStudio Server job at OSC

For more detailed instructions of the first steps,
see [this section](06-R.html#rstudio-at-OSC) from our intro to R session.

- Login to OSC at <https://ondemand.osc.edu>.

- Click on `Interactive Apps` (top bar) > `RStudio Server (Owens and Pitzer)`.

- Fill out the form as shown [here](slides/03-OSC-slides.html#rstudio_server_job).

- Once your job has started, click `Connect to RStudio Server`.

- You should automatically be in your personal dir inside the dir
  `/fs/project/PAS0471/workshops/2020-12_micro`.   
  
  (In the *Files* pane in RStudio, you should see your `.Rproj` file. 
  If not, type `setwd('/fs/project/PAS0471/workshops/2020-12_micro')`
  and then click on the dir with your name in the *Files* pane).

- Now, click on the `markdown` directory in the *Files* pane,
  and open the file `06-reads-to-ASV.Rmd`. That's this file!

-----

## Step 1: Getting Started

### Install and load packages

To save time, we have already installed all the necessary R packages at OSC
into a custom library.
To add this library for the current R session:

```{r libload}
.libPaths(new = '/fs/project/PAS0471/.R/4.0/')
```

Then, load the packages:

```{r}
packages <- c("ggplot2", "gridExtra",
              "dada2", "phyloseq", "DECIPHER", "phangorn")
pacman::p_load(char = packages)

# If you wanted to install and load these packages yourself,
# just make sure you have the pacman package installed:
## install.packages('pacman')
# Then, the code above would work, as it will install pacakes as needed.
```

### Set the file paths

```{r}
# Dir with input fastq files:
indir <- "processed_data/fastq_trimmed/second_trim/"

# Dir for output:
outdir <- 'analysis/ASV_inference/'
dir.create(outdir, recursive = TRUE)

# Fasta file with training data:
# (Check for an up-to-date version at <https://benjjneb.github.io/dada2/training.html>)
tax_key <- "data/ref/silva_nr99_v138_train_set.fa" 

# File with sample metadata:
sample_names_file <- "metadata/sample_meta.txt"
```

### Assign files to forward and reverse reads 

For forward and reverse fastq filenames which have the following name format: `<SAMPLENAME>_R1_001.fastq` and `<SAMPLENAME>_R2_001.fastq`.

```{r}
fnFs <- sort(list.files(indir, pattern="_R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(indir, pattern="_R2_001.fastq.gz", full.names = TRUE))
```

### Extract sample names

Extract the sample names from the `.fastq` file names, assuming file names have format: `<SAMPLENAME>_XXX.fastq`.

The resulting `sample.names` should match the SampleID column in the `sample_data_example` file.

```{r}
sample.names <- sapply(strsplit(basename(fnFs), "-V4-V5_"), `[`, 1)
```

-----

## Step 2: QC

### Plot sequence quality data

You can generate and evaluate plots individually:

```{r}
plotQualityProfile(fnFs[1:2])
```

<br>

Or generate pdf files to inspect multiple plots at once:

```{r}
ii <- sample(length(fnFs), 3)
pdf("Error_profiles.pdf")
par(mfrow=c(2,3))
for (i in ii) { print (plotQualityProfile(fnFs[i]) + ggtitle("Fwd")) }
ii <- sample(length(fnRs), 3)
for (i in ii) { print (plotQualityProfile(fnRs[i]) + ggtitle("Rev")) }
dev.off()
```

<details>
<summary>
&nbsp;  `r icon::fa("info-circle")` &nbsp; More on QC of fastq files
</summary>
It is a good idea to run the
[`fastqc`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)
program on your fastq files for more extensive QC.
This is a stand-alone program that is easy to run from the command-line.
When you have many samples, as is often the case,
`fastqc`'s results can moreover be nicely summarized using
[`multiqc`](https://multiqc.info/.
In the interest of time, we skipped these steps during this workshop.
</details>


-----

## Step 3: Trimming

The `truncLen` parameters should be based on sequence quality visualized above.
It is suggested to trim first 10 nucleotides of each read,
empirical observations across Illumina datasets show these positions are likely to contain pathological errors. Trimming length could be different for R1 and R2,
but ideally not too drastically so, in order to maximize merging pairs in subsequent steps.

First, define paths for trimming:

```{r}
filtFs <- file.path(output, "filtered", paste0(sample.names, "_F_filt.fastq"))
filtRs <- file.path(output, "filtered", paste0(sample.names, "_R_filt.fastq"))
```

<br>

Then, perform the trimming:

```{r}
print("filter and Trimming")
Sys.time()
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, trimLeft=10, truncLen=c(250,210), maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=F, compress=FALSE, multithread=TRUE, verbose=TRUE) 
print("done")
Sys.time()
```


-----

## Step 4: Dereplication & Error Training

```{r}
derepFs <- derepFastq(filtFs, verbose = FALSE)
derepRs <- derepFastq(filtRs, verbose = FALSE)

print("Learning errors:")
Sys.time()
errF <- learnErrors(derepFs, multithread = TRUE, verbose = TRUE)
errR <- learnErrors(derepRs, multithread = TRUE, verbose = TRUE)
print("Done.")
Sys.time()

names(derepFs) <- sample.names
names(derepRs) <- sample.names
```

<br>

Plot errors to verify that error rates have been reasonable well-estimated.
Observe the fit between observed error rates (points) and fitted error rates (lines):

```{r}
plotErrors(errF, nominalQ = TRUE)
plotErrors(errR, nominalQ = TRUE)
```

-----

## Step 5: Infer ASVs

We will run the dada algorithm to infer ASVs.

This step is more computationally intensity. Indepedant inference by sample (pool=F) will keep memory requirements linear to sample number. Pooling will increase computation time but improve detection of rare variants seen once or twice in an individual sample but many times across all samples.

```{r}
print("inferring ASVs (running dada algorithm)")
Sys.time()
dadaFs <- dada(derepFs, err = errF, multithread = T, pool = FALSE)
dadaRs <- dada(derepRs, err = errR, multithread = T, pool = FALSE)
print("done")
Sys.time()

#inspect object
dadaFs[[1]]
```

-----

## Step 6: Merge Read Pairs

In this step, we will merge read pairs and generate sequence table of paired ASVs.

```{r}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs,
                      verbose=TRUE)

seqtab.all <- makeSequenceTable(mergers)
dim(seqtab.all)
```

-----

## Step 7: Summarize Sequences and Remove Chimeras

```{r}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab.all)))

#If you need to remove sequences of a particular length (e.g. too long):
#seqtab2 <- seqtab[,nchar(colnames(seqtab.all)) %in% seq(250,256)]

# Remove chimeras:
seqtab <- removeBimeraDenovo(seqtab.all,
                             method = "consensus",
                             multithread = TRUE,
                             verbose = TRUE)
dim(seqtab)

# Proportion of removed sequences
sum(seqtab) / sum(seqtab.all)

# Save seqtab file as an RDS object
saveRDS(seqtab, file = file.path(outdir, "seqtab_V4.rds"))
```


-----


## Step 8: Generate a Summary Table

In this step, we will generate a summary table of the number of sequences
processed and outputs of different steps of the pipeline.

This information is generally used to further evaluate characteristics and
quality of the run, sample-to-sample variation,
and resulting sequencing depth for each sample. 

```{r}
getN <- function(x) sum(getUniques(x))

track <- cbind(out,
               sapply(dadaFs, getN),
               sapply(dadaRs, getN),
               sapply(mergers, getN),
               rowSums(seqtab))

# If processing a single sample, remove the sapply calls:
# e.g. replace sapply(dadaFs, getN) with getN(dadaFs)

colnames(track) <- c("input", "filtered", "denoisedF",
                     "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)

outfile_track <- file.path(outdir, 'track.txt')
write.table(track, file = outfile_track)
```

-----

## Step 9: Assign Taxonomy to ASVs

Depending on the marker gene and the data, you will choose the appropriate file for this step. Several files have been formatted for taxony assignments in DADA2 pipeline and are available in the following website: https://benjjneb.github.io/dada2/index.html 

```{r}
print("Assigning taxa to ASVs")
Sys.time()
taxa <- assignTaxonomy(seqtab, tax_key, multithread = TRUE)
colnames(taxa) <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus")
print("done")
Sys.time()
```

-----

## Step 10: Phylogenetic Tree Estimation

A phylogenetic tree can be estimated for the sequence data you generated. Depending on the number of ASVs recovered and the phylogenetic tree algorithm of choice, this step could take several days. Simpler trees will be less computationally intensive. Depending on the marker gene you are working on, you might choose or not to use this option. 

```{r, eval=FALSE}
#seqtab<- readRDS("seqtab_V4.rds")
seqs <- getSequences(seqtab)
names(seqs) <- seqs # This propagates to the tip labels of the tree. At this stage ASV labels are full ASV sequence
alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA, iterations = 5, refinements = 5)

print("Computing pairwise distances from ASVs")
Sys.time()
phang.align <- phyDat(as(alignment, "matrix"), type="DNA")
dm <- dist.ml(phang.align)
treeNJ <- NJ(dm) # Note, tip order != sequence order
fit = pml(treeNJ, data=phang.align)
print("done")
Sys.time()

fitGTR <- update(fit, k=4, inv=0.2)
print("done")
Sys.time()

print("Computing likelihood of tree")
Sys.time()
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                      rearrangement = "stochastic", control = pml.control(trace = 0))
print("done")
Sys.time()
```



-----

## Step 11: Generate Output Files

In this step, we will generate output files formatted for downstream analysis in phyloseq

Data will be extracted from the DADA2 outputs and formatted for downstream analysis and data inspection. The last step of this pipeline saves a phyloseq object as an .RDS file and can be imported directly in phyloseq. 

```{r}
# Rename ASV names: 
asv_seqs<- colnames(seqtab)
asv_headers<- vector(dim(seqtab)[2], mode = "character")

for (i in 1:dim(seqtab)[2]) {
	asv_headers[i] <- paste(">ASV", i, sep = "_")
}


# Write ASV fasta for final ASV seqs.
#This fasta file can also be used for phylogenetic tree inference in different platforms
asv_fasta<- c(rbind(asv_headers, asv_seqs))

outfile_fasta <- file.path(outdir, 'ASVs.fa')
write(asv_fasta, outfile_fasta)


# Count table:
asv_tab <- t(seqtab)
row.names(asv_tab) <- sub(">", "", asv_headers)

outfile_tab <- file.path(outdir, 'ASVs_counts.tsv')
write.table(asv_tab, outfile_tab, 
            sep = "\t", quote = FALSE, col.names = NA)


# Tax table:
asv_tax <- taxa
row.names(asv_tax) <- sub(">", "", asv_headers)

outfile_tax <- file.path(outdir, 'ASVs_taxonomy.tsv')
write.table(asv_tax, outfile_tax,
            sep = "\t", quote = FALSE, col.names = NA)


# Read files back in:
count_tab <- read.table(outfile_tab,
                        header = TRUE, row.names = 1,
                        check.names = FALSE, sep = "\t")

tax_tab <- as.matrix(
  read.table(outfile_tax,
             header = TRUE, row.names = 1,
             check.names = FALSE, sep = "\t")
  )


# Load and prepare sample metadata:
sample_df <- read.table(file = sample_names_file,
                        sep = "\t", header = TRUE)
colnames(samdf)[1]<- "SampleID"
rownames(samdf)<- samdf$SampleID


# Build the final phyloseq object:
# We will not do this now, but a phylogenetic tree can also be added to the phyloseq object.
ps <- phyloseq(otu_table(count_tab, taxa_are_rows = TRUE),
               sample_data(sample_df),
               tax_table(tax_tab))

outfile_ps <- file.path(outdir, 'ps_V4.rds')
saveRDS(ps, file = outfile_ps)

print("done")
```

-----

## Resources

- [Callahan et al. 2006: "Bioconductor Workflow for Microbiome Data Analysis: from raw reads to community analyses"](https://f1000research.com/articles/5-1492/v2)
- [`dada2` documentation and tutorials](https://benjjneb.github.io/dada2/index.html )
- [Taxonomic references for `dada2`](https://benjjneb.github.io/dada2/training.html)
- [`cutadapt` documentation and tutorials](https://cutadapt.readthedocs.io/en/stable/index.html)


-----

## Bonus: Submit script as an OSC job

- Script `dada2.sh`:

```sh
#PBS -l walltime=100:00:00
#PBS -l nodes=1:ppn=28
#PBS -A PAS1548

module load gnu/9.1.0
module load mkl/2019.0.3
module load R/3.6.1
cd /fs/scratch/PAS1548/BaseSpace_out/11-21-19/V4_V5/scripts
Rscript dada2_V4.R
```

