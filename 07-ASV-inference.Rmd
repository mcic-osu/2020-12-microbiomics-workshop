---
title: "<br>Workflow part II:<br>ASV Inference and Taxon Assignment"
output:
  rmarkdown::html_document:
    theme: cerulean
    highlight: tango
    code_download: true
    toc: true
    toc_float: true
    css: my.css
editor_options: 
  chunk_output_type: console
---

```{r knitr_options, echo=FALSE}
root_dir <- "/fs/project/PAS0471/workshops/2020-12_micro/$USER"
#knitr::opts_knit$set(root.dir = root_dir) # UNCOMMENT AT OSC

knitr::opts_chunk$set(results='hide',
                      eval = FALSE, # CHANGE TO TRUE TO RUN UPON COMPILING
                      class.source="r_code",
                      class.output="r_output",
                      class.warning="r_warning",
                      class.message="r_warning",
                      class.error="r_error")
```

<br>

-----

## Goals

Process metabarcoding reads, post-adapter removal, including:

- Evaluation of read quality
- Quality trimming and filtering
- Error correction, denoising
- ASV inference
- Read merging
- Taxonomy assignment

-----

## Before We Get Started

### Notes

- This documented was adapted from [Callahan et al. 2006](https://f1000research.com/articles/5-1492/v2)
  by Matthew Willman, with further edits by Soledad Benitez Ponce and Jelmer Poelstra.
  
- To convert an `Rmd` (R Markdown) file to an R script,
  type the following in an R console:
  `knitr::purl(input="<filename>.Rmd")`.
  (You can download this `Rmd` file by clicking the `Code` button
  in the top-right of this page -- but we will open it at OSC.)

### Start an RStudio Server job at OSC

For more detailed instructions of the first steps,
see [this section](06-R.html#rstudio-at-OSC) from our intro to R session.

- Login to OSC at <https://ondemand.osc.edu>.

- Click on `Interactive Apps` (top bar) > `RStudio Server (Owens and Pitzer)`.

- Fill out the form as shown [here](slides/03-OSC-slides.html#rstudio_server_job).

- Once your job has started, click `Connect to RStudio Server`.

- You should automatically be in your personal dir inside the dir
  `/fs/project/PAS0471/workshops/2020-12_micro`,
  with your RStudio Project open.
  You can see whether a Project is open and which one
  in the top-right of your screen:
  
  <p align="center">
  <img src=img/rproj-open.png width="130">
  Here, the project `jelmer` is open. Your project name is also your username.
  </p>

  If your Project isn't open, click on the icon to open it:
  
  <p align="center">
  <img src=img/rproj-dropdown.png width="250">
  </p>

  
- Now, click on the `markdown` directory in the *Files* pane,
  and open the file `07-ASV-inference.Rmd`. That's this file!

-----

## Step 1: Getting Started

### Install and load packages

To save time, we have already installed all the necessary R packages at OSC
into a custom library.
To add this library for the current R session:

```{r libload}
.libPaths(new = '/fs/project/PAS0471/.R/4.0/')
```

Then, load the packages:

```{r}
packages <- c("tidyverse", "gridExtra", "dada2",
              "phyloseq", "DECIPHER", "phangorn")
pacman::p_load(char = packages)

# If you wanted to install and load these packages yourself,
# just make sure you have the pacman package installed:
## install.packages('pacman')
# Then, the code above would work, as it will install pacakes as needed.
```

### Set the file paths

```{r}
# Dir with input fastq files:
indir <- 'data/processed/fastq_trimmed/second_trim'

# Dirs for output:
qc_dir <- 'analysis/QC'
filter_dir <- 'data/processed/fastq_filtered'
outdir <- 'analysis/ASV_inference'

# Fasta file with training data:
# (Check for an up-to-date version at <https://benjjneb.github.io/dada2/training.html>)
tax_key <- 'data/ref/silva_nr99_v138_train_set.fa' 

# File with sample metadata:
metadata_file <- 'metadata/sample_meta.txt'
```

### Assign fastq files to forward and reverse reads 

We will assign the fastq files that we processed with `cutadapt` to two vectors:
one with files with forward reads, and one with files with reverse reads.
These files can be distinguished by having "R1" (forward) and "R2" (reverse)
in their names.

```{r}
fastqs_raw_F <- sort(list.files(indir, pattern = '_R1_001.fastq.gz', full.names = TRUE))
fastqs_raw_R <- sort(list.files(indir, pattern = '_R2_001.fastq.gz', full.names = TRUE))

head(fastqs_raw_F)
```

### Get sample IDs

We'll get the sample IDs from the fastq file names and from a file with metadata,
and will check if they are the same.

First we get the metadata into a dataframe:
```{r}
# Load and prepare sample metadata:
metadata_df <- read.table(file = metadata_file, sep = "\t", header = TRUE)

colnames(metadata_df)[1] <- 'SampleID'
rownames(metadata_df) <- metadata_df$SampleID

head(metadata_df)
```

Let's compare the sample IDs from the metadata with the fastq filenames:

```{r}
metadata_df$SampleID

head(basename(fastqs_raw_F))    # basename() strips the dir name from the filename
```

To extract the sample IDs from the fastq file names, we remove everything after
"-V4-V5" from the file names using the `sub()` function:

```{r}
# sub() arguments: sub(pattern, replacement, vector)
# replace with "" -> replace with nothing
sampleIDs <- sub("-V4-V5_.*", "", basename(fastqs_raw_F))
```

We can check whether the IDs from the fastq files and the metadata dataframe
are the same:

```{r}
identical(sort(metadata_df$SampleID), sampleIDs)

setdiff(sort(metadata_df$SampleID), sampleIDs) # REMOVE MISSING SAMPLES?
```

-----

## Step 2: QC

### Plot sequence quality data

You can generate and evaluate plots individually (forward and reverse side-by-side):

```{r}
plotQualityProfile(c(fastqs_raw_F[1], fastqs_raw_R[1]))
```

<br>

Or generate pdf files to inspect multiple plots at once:

```{r}
pdf(file.path(qc_dir, "error_profiles.pdf"))  # Open a pdf file
for (sample_index in 1:3) {                   # Loop through first three file pairs
  print(plotQualityProfile(                   # Print plots into pdf
    c(fastqs_raw_F[sample_index], fastqs_raw_R[sample_index])) # F and R together
    )
}
dev.off()                                    # Close the pdf file
```

<details>
<summary>
&nbsp;  `r icon::fa("info-circle")` &nbsp; More on QC of fastq files
</summary>
It is a good idea to run the
[`fastqc`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)
program on your fastq files for more extensive QC.
This is a stand-alone program that is easy to run from the command-line.
When you have many samples, as is often the case,
`fastqc`'s results can moreover be nicely summarized using
[`multiqc`](https://multiqc.info/.
In the interest of time, we skipped these steps during this workshop.
</details>

-----

## Step 3: Filtering and Quality Trimming

We will now perform quality filtering (removing poor-quality reads)
and trimming (removing poor-quality bases) on the fastq files
using DADA2's `filterAndTrim()` function.

The `truncLen` argument defines the read lengths
(for forward and reverse reads, respectively)
beyond which additional bases should be removed,
and these values should be based on the sequence quality visualized above.
The trimming length can thus be different for forward and reverse reads,
but ideally not too drastically so,
in order to maximize the number of pairs that will merge in a subsequent step.

It is also suggested to trim the first 10 nucleotides of each read
(`trimLeft` argument),
since these positions are likely to contain errors.

The `filterAndTrim()` function will write the filtered and trimmed reads
to new fastq files.
Therefore, we first define the filenames for the new files:

```{r}
fastqs_filt_F <- file.path(filter_dir, paste0(sampleIDs, '_F_filt.fastq'))
fastqs_filt_R <- file.path(filter_dir, paste0(sampleIDs, '_R_filt.fastq'))
```

<br>

Then, perform the trimming:

```{r}
print('Filtering and Trimming...')
Sys.time()  # Print the time to keep track of running time for individual steps 
filter_results <-
  filterAndTrim(fastqs_raw_F, fastqs_filt_F,
                fastqs_raw_R, fastqs_filt_R,
                truncLen = c(250,210),
                trimLeft = 10,
                maxN = 0,
                maxEE = c(2,2),
                truncQ = 2,
                rm.phix = FALSE,
                compress = FALSE, multithread = TRUE, verbose = TRUE) 
print('...Done!')
Sys.time()

head(filter_results)
```

-----

## Step 4: Dereplication & Error Training

The next thing we want to do is “dereplicate” the filtered fastq files.
During dereplication, we condense the data by collapsing together all reads that
encode the same sequence, which significantly reduces later computation times.

```{r}
fastqs_derep_F <- derepFastq(fastqs_filt_F, verbose = FALSE)
fastqs_derep_R <- derepFastq(fastqs_filt_R, verbose = FALSE)

names(fastqs_derep_F) <- sampleIDs
names(fastqs_derep_R) <- sampleIDs
```

The DADA2 algorithm makes use of a parametric error model (`err`) and every
amplicon dataset has a different set of error rates.
The `learnErrors` method learns this error model from the data,
by alternating estimation of the error rates and inference of sample composition
until they converge on a jointly consistent solution.

```{r}
print('Learning errors...')
Sys.time()

err_F <- learnErrors(fastqs_derep_F, multithread = TRUE, verbose = TRUE)
err_R <- learnErrors(fastqs_derep_R, multithread = TRUE, verbose = TRUE)
#saveRDS(err_F, file = file.path(outdir, "err_F.rds"))
#saveRDS(err_R, file = file.path(outdir, "err_R.rds"))

print('...Done!')
Sys.time()
```

<br>

Plot errors to verify that error rates have been reasonable well-estimated.
Observe the fit between observed error rates (points) and fitted error
rates (lines):

```{r}
plotErrors(err_F, nominalQ = TRUE)
plotErrors(err_R, nominalQ = TRUE)
```

-----

## Step 5: Infer ASVs

We will run the dada algorithm to infer Amplicon Sequence Variants (ASVs).

This step is quite computationally intensive,
and we will therefore perform independent inference for each sample (`pool = FALSE`),
which will keep memory usage linear to the sample number.
Pooling will increase memory rquirements and computation time,
but improve detection of rare variants seen once or twice in an individual sample,
but many times across all samples.

```{r}
print('Inferring ASVs (running the dada algorithm)...')
Sys.time()

dada_Fs <- dada(fastqs_derep_F, err = err_F, pool = FALSE, multithread = TRUE)
dada_Rs <- dada(fastqs_derep_R, err = err_R, pool = FALSE, multithread = TRUE)
#saveRDS(dada_Fs, file = file.path(outdir, "dada_Fs.rds"))
#saveRDS(dada_Fs, file = file.path(outdir, "dada_Fs.rds"))

print('...Done.')
Sys.time()
```

Let's inspect one of the resulting objects:

````{r}
dada_Fs[[1]]
```

-----

## Step 6: Merge Read Pairs

In this step, we will first merge the forward and reverse read pairs:
the fragment that we amplified with our primers was short enough
to generate lots of overlap among the sequences from the two directions.

```{r}
mergers <- mergePairs(dada_Fs, fastqs_derep_F,
                      dada_Rs, fastqs_derep_R,
                      verbose = TRUE)
```

<br>

Just like tables can be saved in R using `write.table` or `write.csv`,
R *objects* can be saved using `saveRDS`.
The resulting .rds file can then be loaded into an R environment using `readRDS`.
This is a convenient way to save R objects that require a lot of computation time.

We should not be needing the very large dereplicated sequence objects anymore,
but to be able to quickly restart our analysis from a new R session
if necessary, we now save these objects to RDS files.
And after that, we can safely remove these objects from our environment.

```{r}
saveRDS(fastqs_derep_F, file = file.path(outdir, "fastqs_derep_F.rds"))
saveRDS(fastqs_derep_R, file = file.path(outdir, "fastqs_derep_R.rds"))

rm(fastqs_derep_F, fastqs_derep_R) # Remove objects from environment
```

-----

## Step 7: Construct a Sequence Table

Next, we construct an amplicon sequence variant table (ASV) table:

```{r}
seqtab_all <- makeSequenceTable(mergers)

# The dimensions of the object are the nr of samples (rows) and the nr of ASVs (columns):
dim(seqtab_all)
```

Let's inspect the distribution of sequence lengths:

```{r}
table(nchar(getSequences(seqtab_all)))

# If you need to remove sequences of a particular length (e.g. too long):
# seqtab2 <- seqtab[, nchar(colnames(seqtab_all)) %in% seq(250,256)]
```


-----

## Step 8: Remove Chimeras

Now, we will remove chimeras.
The `dada` algorithm models and removes substitution errors,
but chimeras are another importance source of spurious sequences in amplicon sequencing. Chimeras are formed during PCR amplification.
When one sequence is incompletely amplified, the incomplete amplicon primes the
next amplification step, yielding a spurious amplicon.
The result is a sequence read which is half of one sample sequence and half another.

Fortunately, the accuracy of the sequence variants after denoising makes identifying chimeras simpler than it is when dealing with fuzzy OTUs.
Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant "parent" sequences.

```{r}
seqtab <- removeBimeraDenovo(seqtab_all,
                             method = "consensus",
                             multithread = TRUE,
                             verbose = TRUE)
ncol(seqtab)

# Proportion of retained sequences:
sum(seqtab) / sum(seqtab_all)
```

We will save the `seqtab` object as an RDS file:

```{r}
saveRDS(seqtab, file = file.path(outdir, "seqtab_V4.rds"))
```

-----

## Step 9: Generate a Summary Table

In this step, we will generate a summary table of the number of sequences
processed and outputs of different steps of the pipeline.

This information is generally used to further evaluate characteristics and
quality of the run, sample-to-sample variation,
and resulting sequencing depth for each sample. 

```{r}
# We define a function `getN`, which will get the nr of unique reads for a sample: 
getN <- function(x) sum(getUniques(x))

# We apply the getN function to each element of the dada_Fs, dada_Rs, and mergers objects:
# This gives us a vector with the number of unique reads for each samples,
# for each of these steps:
denoised_F <- sapply(dada_Fs, getN)
denoised_R <- sapply(dada_Rs, getN)
merged <- sapply(mergers, getN)

# We join these vectors together with the "filter_results" dataframe,
# and the number of nonchimeric reads:
nreads_summary <- data.frame(filter_results,
                             denoised_F,
                             denoised_R,
                             nonchim = rowSums(seqtab),
                             row.names = sampleIDs)
colnames(nreads_summary)[1:2] <- c('input', 'filtered')

# Have a look at the first few rows:
head(nreads_summary)

# We'll write this table to file:
write.table(nreads_summary, file = file.path(outdir, 'nreads_summary.txt'),
            sep = "\t", quote = FALSE, row.names = TRUE)
```

-----

## Step 10: Assign Taxonomy to ASVs

Now, we will assign taxonomy to our ASVs.

Depending on the marker gene and the data,
you will have to choose the appropriate reference file for this step.

Several files have been formatted for taxonomy assignments in DADA2 pipeline and are available at the [DADA2 website](https://benjjneb.github.io/dada2/index.html).

```{r}
print('Assigning taxa to ASVs...')
Sys.time()

taxa <- assignTaxonomy(seqtab, tax_key, multithread = TRUE)
colnames(taxa) <- c('Kingdom', 'Phylum', 'Class', 'Order', 'Family', 'Genus')

print('...Done.')
Sys.time()
```

-----

## Step 11: Generate Output Files

In this step, we will generate output files formatted for downstream analysis in *phyloseq*.
Data will be extracted from the DADA2 outputs and formatted for downstream
analysis and data inspection.
The last step of this pipeline saves a *phyloseq* object as an .RDS file,
which can be imported directly by *phyloseq*.

```{r}
# Rename ASVs: 
asv_seqs <- colnames(seqtab)
asv_headers <- paste('>ASV', 1:ncol(seqtab), sep = '_')
```

Next, we will write a fasta file with the final ASV sequences.
This fasta file can also be used for phylogenetic tree inference in different platforms.

```{r}
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, file = file.path(outdir, 'ASVs.fa'))
```

We will create count (OTU) and taxon tables:

```{r}
# Create a count (OTU) table:
otu_df <- t(seqtab)
row.names(otu_df) <- sub('>', '', asv_headers)

# Create a taxon table:
tax_df <- taxa
row.names(tax_df) <- sub(">", "", asv_headers)
```

Build the final *phyloseq* object (we will not do this now,
but a phylogenetic tree can also be added to the phyloseq object):

```{r}
ps <- phyloseq(otu_table(otu_df, taxa_are_rows = TRUE),
               sample_data(metadata_df),
               tax_table(tax_df))

# Write tables and phyloseq object to file:
write.table(otu_df, file.path(outdir, 'ASVs_counts.tsv'),
            sep = "\t", quote = FALSE, col.names = NA)

write.table(tax_df, file.path(outdir, 'ASVs_taxonomy.tsv'),
            sep = "\t", quote = FALSE, col.names = NA)

saveRDS(ps, file = file.path(outdir, 'ps_V4.rds'))
```

Report that we are done!

```{r}
print('Done with ASV inference.')
```

-----

## Resources

- [Callahan et al. 2006: "Bioconductor Workflow for Microbiome Data Analysis: from raw reads to community analyses"](https://f1000research.com/articles/5-1492/v2)
- [`dada2` documentation and tutorials](https://benjjneb.github.io/dada2/index.html )
- [Taxonomic references for `dada2`](https://benjjneb.github.io/dada2/training.html)
- [`cutadapt` documentation and tutorials](https://cutadapt.readthedocs.io/en/stable/index.html)


-----

## Bonus: Phylogenetic Tree Estimation

A phylogenetic tree can be estimated for the sequence data you generated.
Depending on the number of ASVs recovered and the phylogenetic tree algorithm of choice, this step could take several days.
Simpler trees will be less computationally intensive. Depending on the marker gene you are working on, you might choose or not to use this option.

This step can be conducted after Step 10,
and then the phylogeny can be included in the *phyloseq* object in Step 11.

```{r, eval = FALSE}
#seqtab<- readRDS('seqtab_V4.rds')

seqs <- getSequences(seqtab)

# This propagates to the tip labels of the tree.
# At this stage ASV labels are full ASV sequence
names(seqs) <- seqs 
alignment <- AlignSeqs(DNAStringSet(seqs),
                       anchor = NA,
                       iterations = 5,
                       refinements = 5)

print('Computing pairwise distances from ASVs...')
Sys.time()
phang.align <- phyDat(as(alignment, 'matrix'), type = 'DNA')
dm <- dist.ml(phang.align)
treeNJ <- NJ(dm) # Note, tip order is not sequence order
fit = pml(treeNJ, data = phang.align)
print('...Done.')
Sys.time()

print('....')
Sys.time()
fitGTR <- update(fit, k=4, inv=0.2)
print('...Done.')
Sys.time()

print('Computing likelihood of tree...')
Sys.time()
fitGTR <- optim.pml(fitGTR, model='GTR', optInv=TRUE, optGamma=TRUE,
                      rearrangement = 'stochastic', control = pml.control(trace = 0))
print('...Done'.)
Sys.time()
```

-----

## Bonus: Submit script as an OSC job

Extract R code from this document:
```{r, eval = FALSE}
knitr::purl(input = 'markdown/07-ASV-inference.Rmd',
            output = 'scripts/02-dada2_V4.R')
```

Our script `02-dada2.sh` with *SLURM* directives that will submit the R
script from the shell using `Rscript`:

```{bash, eval = FALSE}
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=28
#SBATCH --time=100:00:00
#SBATCH --account=PAS0471

module load gnu/9.1.0
module load mkl/2019.0.3
module load R/4.02

Rscript scripts/02-dada2_V4.R
```

Submit the script:

```{bash, eval = FALSE}
sbatch scripts/02-dada2.sh
```

<br><br>
